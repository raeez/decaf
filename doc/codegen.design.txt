>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=

                         888888 888888    db    8b    d8
                           88   88__     dPYb   88b  d88
                           88   88""    dP__Yb  88YbdP88
                           88   888888 dP""""Yb 88 YY 88

 88   88 88b 88 888888 Yb    dP    db    88     88   88    db    888888 888888 8888b.
 88   88 88Yb88 88__    Yb  dP    dPYb   88     88   88   dPYb     88   88__    8I  Yb 
 Y8   8P 88 Y88 88""     YbdP    dP__Yb  88  .o Y8   8P  dP__Yb    88   88""    8I  dY 
 `YbodP' 88  Y8 888888    YP    dP""""Yb 88ood8 `YbodP' dP""""Yb   88   888888 8888Y"

>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=

                        presents a design for a code generator

                           ================================
                           | Augmenting Semantic Checking |
                           ================================

1. Augmenting the SymbolTable with symbolic addresses

    A SymbolTable is an Algebraic Data Type holding a record of currently
    defined Symbols, as well as the 'type' of the current enclosing scope
    (see src/Decaf/Data/SymbolTable.hs)

    -- | Program symbols are stored in the 'SymbolTable' structure
    data SymbolTable = SymbolTable
        { symbolRecords :: [SymbolRecord]
        , blockType :: BlockType
        } deriving (Show, Eq)

    -- | Individual program symbol entries are stored in the 'SymbolRecord' structure
    data SymbolRecord = VarRec DecafVar 
                      | MethodRec DecafMethod
                      | ArrayRec DecafArr
                      deriving (Show, Eq)

    -- | Symbol representing the various styleof block in Decaf
    data BlockType = ForBlock
                   | IfBlock
                   | MethodBlock DecafType
                   | GlobalBlock
                   | TrivialBlock
                   deriving (Show, Eq)

    Each SymbolTable is utilized as the conanical record for the definition
    of program symbols; however, the SymbolRecord as defined in our
    Semantic Checking phase is inadequate (as-is) for code generation;
    thus, we augment the SymbolRecords to hold key information: the
    Symbolic Address of the given record.

    Symbolic addresses represent placeholder addressing methodologies for
    both local and global program symbols (the only differentiation between
    local and global program symbols is made in the differentiation between
    the global symbol table - i.e. that symbol table at the topmost of the
    SymbolTree, and any subsequent SymbolTable instances (i.e. any child
    of the global SymbolTable in the SymbolTree)). Note: See the Section
    on Zippers for more information on the SymbolTree. A symbolic address
    can take on the form of


        * a symbolic register, denoted as sx, where x is positive
          integer

        * an offset from the global pointer, denoted as gp + x, where x
          is the (positive or negative) integer of the offset in bytes

        * an offset from the frame pointer, denoted as fp + x, where x
          is the (positive or negative) integer of the offset in bytes

        * a static resource label (i.e. in the case of strings)


    Thus, we augment SymbolTable.hs with the following additional structure:


    
2. Augmenting the Checking Phase (src/Decaf/Checker.hs)

    Given the improved design of the SymbolTable and SymbolRecord data structures,
    we can now trivially augment our monadic walk of the AST (see src/Decaf/Checker.hs)
    by assigning monotonically increasing (within a given scope) symbolic addresses;
    this can be done with an internal state counter (as already implemented in Checker.hs)

    At this point in time, we've chosen to delay this implementation in favour of
    tackling some of the more challenging infrastructure required for Control Flow
    graphs (see Zippers)

                               ======================
                               | Intermediate Codes |
                               ======================

1. AST - The Abstract Syntax Tree


    The first intermediate code our compiler generates is the Abstract Syntax
    Tree (see src/Decaf/AST.hs); the Abstract Syntax Tree is a high-level
    representation of the Decaf Program to be compiled. In it, we represent
    all high-level program constructs as first class Algebraic Datatypes
    in Haskell. i.e. a Statement in the AST would be represented as:


    data DecafExpr = DecafExpr Term Expr' DecafPosition
                   | DecafLocExpr DecafLoc DecafPosition
                   | DecafMethodExpr DecafMethodCall DecafPosition
                   | DecafLitExpr DecafLiteral DecafPosition
                   | DecafBinExpr DecafExpr DecafBinOp DecafExpr DecafPosition
                   | DecafNotExpr DecafExpr DecafPosition
                   | DecafMinExpr DecafExpr DecafPosition
                   | DecafParenExpr DecafExpr DecafPosition
                   deriving (Show, Eq)


    This translates to each high level construct (Program, Location, Field,
    Variable, Array, Method, Statement, Argument, Expression, Operator, Type
    etc.) having it's own representation as an Algebraic Data Type. The
    benefits of doing so are two-fold:


        * Haskell's Hindley Milner type system guarantees only valid
          instantiations of Abstract Syntax Trees

        * Walking the Abstract Syntax Tree is accentuated with
          Constructor-styled pattern matching

        * We gain Haskell's type inference abilities as an advantage;
          non-exhaustive pattern's (and hence related bugs) are caught
          by the host language's compiler


    The inherent disadvantages of constructing the AST as a recursive
    Algebraic Data Type reveals itself in:


        * having to write a lot of redundant, case-specific code; i.e.
          each Node of the Abstract Syntax Tree has to have it's own
          'walk' function, 'pretty print' function etc.

        * being unable to express generic invariants

    This results in quite verbose code (AST.hs clocks in at 447 lines of
    code!). These disadvantages were somewhat offset by the use of Haskell's
    typeclasses; despite having to write implementations for each node
    of the AST, we can have a unified interface for dealing nodes:

    class ASTNode a where
        pos :: a -> DecafPosition   -- return the source position
        pp :: a -> String           -- pretty print
        treeify :: a -> Tree String -- turn into a generic tree

    The interface corresponds to the original source Position of the AST
    Node, a pretty printer function and custom 'treeify' function that
    casts the structured AST into an unstructured 'generic' tree (we
    utilize the generic tree in pretty printing, graphing etc.)


2. LIR - The Low-level Intermediate Representation


    The second intermediate code used in code generation is the
    Low-Level Intermediate Representation, or LIR. In stark comparison to
    the AST, the LIR has an almost 1-to-1 correspondance to x86_64 machine
    instructions; essentially, our LIR can be classified as a linearized
    series of Trees, each tree represented as a double, triple or quadruple;
    a single component making up each double|triple|quadruple can itself be
    a quadruple or triple (i.e. in the case of assignment of expressions).    
    A few examples of the LIR trees as doubles | triples | quadruples:


    (1)  + s1 s1 0x3      -- a quadruple
    (2)  goto L3          -- a double
    (3)  = s7 [fp - 80]   -- a triple, with a nested triple


    representing (1) adding the integer literal '0x3' to the value stored in
    symbolic register 's2', and storing the result in 's1'; (2) an unconditional
    jump to named label and (3) an assignment of the value found at relative
    memory 80 bytes below the frame pointer into symbolic register 's7'.
    It's important to note that in LIR


        - all scalar values are represented by symbolic registers
          (naive register allocaction occurs later)

        - all array locations are represented as offsets from a global
          pointer (represented symbolically as gp, to be resolved later
          on)

        - conditionals are represented as both 'if-cond' and as
          individual 'cmp-jmp' instructions; the purpose of having both
          a higher level and lower level construct in LIR is to assist using
          the high level constructs in LIR prior to doing control flow
          analysis, and then transitioning to using the lower level
          constructs post control flow analysis.

        - all array bounds accesses are surrounded by guards (i.e. to
          check if the result is out of bounds)

    LIR is purposefully low-level, such that generating x86_64 code from
    it is incredibly simple; most of the work is done lowering the
    abstraction from the AST to the LIR, and subsequently in control
    flow analysis. Thus, the following is performed on lowering to LIR:


        * Generation of function prologue and epilogue, and
          register/value passing

        * Reduction of high level looping constructs to if comparisons
          and looping structures

    The grammar for our LIR is given below:

    LIRProg = (Maybe Label) [LIRUnit]
    LIRUnit = (Maybe Label) begin LIRInstructions end
    LIRInstructions = (Maybe Label)
    LIRInst = RegAssignInst
            | CondAssignInst
            | StoreInst
            | LoadInst
            | GotoInst
            | IfInst
            | CallInst
            | ReturnInst
            | SequenceInst
            | Label LIRInst
    RegAssignInst = RegName Expr            -- copy expr to RegName
                  | RegName                 -- copy Integer bytes from (RegName + Integer), to RegName
                        RegName (Integer, Integer)
                        Operand -- register

    CondAssignInst = RegName Operand RegName
    IfInst = RelExpr Label
    StoreInst = MemAddr Operand
    LoadInst = RegName MemAddr
    GotoInst = goto Label           -- explicit label
             | goto Regname Offset  -- mem addr in reg
    CallInst = (Maybe RegName) ProcName RegName
             | (Maybe RegName) RegName RegName
    Operand = RegName
            | IntegerLiteral
    MemAddr = RegName
            | RegName RegName -- register offset addressing
            | RegName Offset  -- literal offset addressing
    RegName = String
    Label = String
    Expr = Operand BinOper Operand
         | UnaryOper Operand
         | Operand
    RelExpr = Operand RelOpr Operand
            | ! Operand
    BinOpr = +
           | -
           | *
           | /
           | mod
           | RelOper
           | shl
           | shr
           | shra
           | and
           | or
           | xor
    RelOpr = ==
           | !=
           | <
           | >
           | <=
           | >=

                       =========================
                       | Control Flow Analysis |
                       =========================


    While at this stage our Control Flow Analysis pass does little in
    the way of analysis, we do take care to structure the pass
    correctly. Namely:


        * we first divy up the sequence of LIR instructions (and their
          subsequent trees) into Basic Blocks, where a basic block is a
          sequence of LIR instructions with the invariant that the only
          way to enter the basic block is at the first LIR instruction
          (namely the label), and the only way to leave the basic block
          is by way of the last instruction (which may or may not be a
          jump)

        * we then insert the basic blocks into a zipper, representing
          our Control Flow Graph, where edges in the graph represent
          jumps

        * we then do some *minimal* control flow analysis, namely to
          implement short-circuiting (i.e. we flatten our boolean
          expressions into nodes in the zipper, such that we can
          generate new nodes representing short circuit conditional
          logic.

       * Finally, we linearize the zipper, taking care to place basic
         blocks in linear order, such that we have all false blocks after
         a conditional, and all true blocks after said false blacks.
         Note that new conditionals 

                       =============================
                       | Naive register Allocation |
                       =============================

    Where a full implementation of our compiler may utilize graph
    coloring techniques, at this stage we will simply implement a naive
    register allocator; the naive register allocator works as follows:

        * given a stream of linearized low-level LIR (i.e. post Control
          flow), we naively reduce all symbolic regsters to locations on
          the stack, along with the generation of corresponding loads
          and stores.

    This is an incredibly simple/trivial phase; not much work is done
    (indeed, all the variables are placed on the stack, decrementing the
    frame pointer as we go)

                       =========================
                       | Naive Code Generators |
                       =========================

    Finally, our design accounts for a naive code generator that is able
    to take a linearized stream of LIR post naive register allocator; we
    then pattern match our LIR (with it's near 1-to-1 mapping) to x86_64
    instructions; we also traverse our SymbolTree for any stored
    strings, and bring all the constructs together into an Assembly
    file. In order to this, we have designed (but not implemented) an
    ASM monad, such that we can write the following code:

            snippet = do
                        mov "r12" "r13"
                        mov "r16" (mem "r13")
                        etc.

    which maps to generated strings representing ASM. thus, we are able
    to utilize the Haskell type system in order to ensure that we are
    notgenerating invalid ASM.


                       ===========
                       | Zippers |
                       ===========


    Much of the past week has been spent investigating the
    implementation of functional data structures in order to a)
    implement control flow graphs and b) speed up our (previously slow)
    implementation of nested symbol tables in a symbol tree; as such, we
    have implemented a derivative data structure called the Zipper (see
    src/Decaf/Data/Zipper.hs). A Zipper is essentially a graph with a
    focus; i.e. a graph where we have chosen a specific element to be
    the root, and as such we represent the entire tree spanning outwards
    from the given focus; as such, we can arbitrarily navigate such a graph
    , with the invariant that local operations can be done in close in
    amortized O(1) time.

    We have implemented, and subsequently rewritten a large portion of
    exsiting code to use the zippers.
