>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=

                         888888 888888    db    8b    d8
                           88   88__     dPYb   88b  d88
                           88   88""    dP__Yb  88YbdP88
                           88   888888 dP""""Yb 88 YY 88

 88   88 88b 88 888888 Yb    dP    db    88     88   88    db    888888 888888 8888b.
 88   88 88Yb88 88__    Yb  dP    dPYb   88     88   88   dPYb     88   88__    8I  Yb 
 Y8   8P 88 Y88 88""     YbdP    dP__Yb  88  .o Y8   8P  dP__Yb    88   88""    8I  dY 
 `YbodP' 88  Y8 888888    YP    dP""""Yb 88ood8 `YbodP' dP""""Yb   88   888888 8888Y"

>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=>>=

         presents the writeup covering the implementation of the code generator

                                   ===========
                                   |  Redux  |
                                   ===========

  Our team's implementation of the code generator for the can be the decaf language
can be split cleanly into three distinct parts; the implementation of our compiler's
*Low Level Intermediate Representation* (LIR for short), the design and implementation
of the *Translator* (a program that converts our abstract high-level language into the
lower form; and finally, the augmentation of the existing Symbol Tables and Symbol Trees.
The x86_64 assembly code was generated as a simple map from LIR to assembly, implemented
in Decaf.IR.ASM.

    Since LIR represents a language in an incredibly low-level (indeed, almost
    machine-specific) form, most of the time spent in implementation was directed
    to the design and implemntation of the LIR, and the corresponding transform:
                      
                      (SymbolTable x AST) -> (SymbolTable x LIR)

    While our initial design accounted for the complexity of such a transform, we
    ultimately found this step to be challenging; not so much on a conceptual level,
    but rather on the raw mechanical level; the limitations of programming in an
    entirely pure (particularly non-mutable) were brought out in full force;

    Specifically, we encountered great difficulty in designing a
    workflow that:

        * Preserved state [1] correctly accross our (increasingly large)
          pipeline of functions

        * Provided arbitrary graph traversals [2] (both for control flow
          graph construction, and for future analyses)

        * Was well-factored (i.e. was not repetitive)

    Given the reality that we were faced with (only Scott and Raeez able to
    contribute to the codebase), we spent the majority of ou implementation
    time on the Translator (see Decaf.Translator), with satisfacotry results;
    LIR is a solid foundation on which to build the optimization engine for
    our compiler; however, the complexity in the Translator stage came at a
    cost; our implementation of the subsequent components of the pipeline (the
    map from LIR -> ASM, and the naive register allocator) recieved far
    less time then then was due; the result is that both components may
    see severe refactoring in the coming weeks.

[1] We had particular difficulty with appropriately keeping track of identity and 
namespace accross the pipeline; this led to the proliferation of numerous repetitive
'counting' routinesâ€”for labeling nodes in a graph, for labeling registers symbolically
etc. More on this later.

[2] The SymbolTree (a zipper of SymbolTable; see Decaf.IR.SymbolTable) was dealt with
in an incredibly natuaral and powerful language; traversing the tree functionally can
be performed as a series of 'direction' functions:

              content . tree . select 3 . select 5 .  (SymbolTree)

(The above example specifies navigation to the 5th child of the current position,
followed by the 3rd child of the subsequent subtree, followed by a retrieval of the
content stored at that point in the tree); beyond trees, however, we found it
*incredibly* challenging to represent arbitrary *graphs*, but more on this later.


                                ================
                                |  Translator  |
                                ================

    Each SymbolTable is utilized as the conanical record for the definition
    of program symbols; however, the SymbolRecord as defined in our
    Semantic Checking phase is inadequate (as-is) for code generation;
    thus, we augment the SymbolRecords to hold key information: the
    Symbolic Address of the given record.

        * a symbolic register, denoted as sx, where x is positive
          integer

        * an offset from the global pointer, denoted as gp + x, where x
          is the (positive or negative) integer of the offset in bytes

        * an offset from the frame pointer, denoted as fp + x, where x
          is the (positive or negative) integer of the offset in bytes

        * a static resource label (i.e. in the case of strings)

2. Augmenting the Checking Phase (src/Decaf/Checker.hs)

    Given the improved design of the SymbolTable and SymbolRecord data structures,
    we can now trivially augment our monadic walk of the AST (see src/Decaf/Checker.hs)
    by assigning monotonically increasing (within a given scope) symbolic addresses;
    this can be done with an internal state counter (as already implemented in Checker.hs)

    At this point in time, we've chosen to delay this implementation in favour of
    tackling some of the more challenging infrastructure required for Control Flow
    graphs (see Zippers)

                               ======================
                               | Intermediate Codes |
                               ======================

1. AST - The Abstract Syntax Tree


    The first intermediate code our compiler generates is the Abstract Syntax
    Tree (see src/Decaf/AST.hs); the Abstract Syntax Tree is a high-level
    representation of the Decaf Program to be compiled. In it, we represent
    all high-level program constructs as first class Algebraic Datatypes
    in Haskell. i.e. a Statement in the AST would be represented as:


    data DecafExpr = DecafExpr Term Expr' DecafPosition
                   | DecafLocExpr DecafLoc DecafPosition
                   | DecafMethodExpr DecafMethodCall DecafPosition
                   | DecafLitExpr DecafLiteral DecafPosition
                   | DecafBinExpr DecafExpr DecafBinOp DecafExpr DecafPosition
                   | DecafNotExpr DecafExpr DecafPosition
                   | DecafMinExpr DecafExpr DecafPosition
                   | DecafParenExpr DecafExpr DecafPosition
                   deriving (Show, Eq)


    This translates to each high level construct (Program, Location, Field,
    Variable, Array, Method, Statement, Argument, Expression, Operator, Type
    etc.) having it's own representation as an Algebraic Data Type. The
    benefits of doing so are two-fold:


        * Haskell's Hindley Milner type system guarantees only valid
          instantiations of Abstract Syntax Trees

        * Walking the Abstract Syntax Tree is accentuated with
          Constructor-styled pattern matching

        * We gain Haskell's type inference abilities as an advantage;
          non-exhaustive pattern's (and hence related bugs) are caught
          by the host language's compiler


    The inherent disadvantages of constructing the AST as a recursive
    Algebraic Data Type reveals itself in:


        * having to write a lot of redundant, case-specific code; i.e.
          each Node of the Abstract Syntax Tree has to have it's own
          'walk' function, 'pretty print' function etc.

        * being unable to express generic invariants

    This results in quite verbose code (AST.hs clocks in at 447 lines of
    code!). These disadvantages were somewhat offset by the use of Haskell's
    typeclasses; despite having to write implementations for each node
    of the AST, we can have a unified interface for dealing nodes:

    class ASTNode a where
        pos :: a -> DecafPosition   -- return the source position
        pp :: a -> String           -- pretty print
        treeify :: a -> Tree String -- turn into a generic tree

    The interface corresponds to the original source Position of the AST
    Node, a pretty printer function and custom 'treeify' function that
    casts the structured AST into an unstructured 'generic' tree (we
    utilize the generic tree in pretty printing, graphing etc.)


2. LIR - The Low-level Intermediate Representation


    The second intermediate code used in code generation is the
    Low-Level Intermediate Representation, or LIR. In stark comparison to
    the AST, the LIR has an almost 1-to-1 correspondance to x86_64 machine
    instructions; essentially, our LIR can be classified as a linearized
    series of Trees, each tree represented as a double, triple or quadruple;
    a single component making up each double|triple|quadruple can itself be
    a quadruple or triple (i.e. in the case of assignment of expressions).    
    A few examples of the LIR trees as doubles | triples | quadruples:


    (1)  + s1 s1 0x3      -- a quadruple
    (2)  goto L3          -- a double
    (3)  = s7 [fp - 80]   -- a triple, with a nested triple


    representing (1) adding the integer literal '0x3' to the value stored in
    symbolic register 's2', and storing the result in 's1'; (2) an unconditional
    jump to named label and (3) an assignment of the value found at relative
    memory 80 bytes below the frame pointer into symbolic register 's7'.
    It's important to note that in LIR


        - all scalar values are represented by symbolic registers
          (naive register allocaction occurs later)

        - all array locations are represented as offsets from a global
          pointer (represented symbolically as gp, to be resolved later
          on)

        - conditionals are represented as both 'if-cond' and as
          individual 'cmp-jmp' instructions; the purpose of having both
          a higher level and lower level construct in LIR is to assist using
          the high level constructs in LIR prior to doing control flow
          analysis, and then transitioning to using the lower level
          constructs post control flow analysis.

        - all array bounds accesses are surrounded by guards (i.e. to
          check if the result is out of bounds)

    LIR is purposefully low-level, such that generating x86_64 code from
    it is incredibly simple; most of the work is done lowering the
    abstraction from the AST to the LIR, and subsequently in control
    flow analysis. Thus, the following is performed on lowering to LIR:


        * Generation of function prologue and epilogue, and
          register/value passing

        * Reduction of high level looping constructs to if comparisons
          and looping structures

    The grammar for our LIR is given below:

    LIRProg = (Maybe Label) [LIRUnit]
    LIRUnit = (Maybe Label) begin LIRInstructions end
    LIRInstructions = (Maybe Label)
    LIRInst = RegAssignInst
            | CondAssignInst
            | StoreInst
            | LoadInst
            | GotoInst
            | IfInst
            | CallInst
            | ReturnInst
            | SequenceInst
            | Label LIRInst
    RegAssignInst = RegName Expr            -- copy expr to RegName
                  | RegName                 -- copy Integer bytes from (RegName + Integer), to RegName
                        RegName (Integer, Integer)
                        Operand -- register

    CondAssignInst = RegName Operand RegName
    IfInst = RelExpr Label
    StoreInst = MemAddr Operand
    LoadInst = RegName MemAddr
    GotoInst = goto Label           -- explicit label
             | goto Regname Offset  -- mem addr in reg
    CallInst = (Maybe RegName) ProcName RegName
             | (Maybe RegName) RegName RegName
    Operand = RegName
            | IntegerLiteral
    MemAddr = RegName
            | RegName RegName -- register offset addressing
            | RegName Offset  -- literal offset addressing
    RegName = String
    Label = String
    Expr = Operand BinOper Operand
         | UnaryOper Operand
         | Operand
    RelExpr = Operand RelOpr Operand
            | ! Operand
    BinOpr = +
           | -
           | *
           | /
           | mod
           | RelOper
           | shl
           | shr
           | shra
           | and
           | or
           | xor
    RelOpr = ==
           | !=
           | <
           | >
           | <=
           | >=

                       =========================
                       | Control Flow Analysis |
                       =========================


    While at this stage our Control Flow Analysis pass does little in
    the way of analysis, we do take care to structure the pass
    correctly. Namely:


        * we first divy up the sequence of LIR instructions (and their
          subsequent trees) into Basic Blocks, where a basic block is a
          sequence of LIR instructions with the invariant that the only
          way to enter the basic block is at the first LIR instruction
          (namely the label), and the only way to leave the basic block
          is by way of the last instruction (which may or may not be a
          jump)

        * we then insert the basic blocks into a zipper, representing
          our Control Flow Graph, where edges in the graph represent
          jumps

        * we then do some *minimal* control flow analysis, namely to
          implement short-circuiting (i.e. we flatten our boolean
          expressions into nodes in the zipper, such that we can
          generate new nodes representing short circuit conditional
          logic.

       * Finally, we linearize the zipper, taking care to place basic
         blocks in linear order, such that we have all false blocks after
         a conditional, and all true blocks after said false blacks.
         Note that new conditionals 

                       =============================
                       | Naive register Allocation |
                       =============================

    Where a full implementation of our compiler may utilize graph
    coloring techniques, at this stage we will simply implement a naive
    register allocator; the naive register allocator works as follows:

        * given a stream of linearized low-level LIR (i.e. post Control
          flow), we naively reduce all symbolic regsters to locations on
          the stack, along with the generation of corresponding loads
          and stores.

    This is an incredibly simple/trivial phase; not much work is done
    (indeed, all the variables are placed on the stack, decrementing the
    frame pointer as we go)

                           ======================
                           | Mapping LIR to ASM |
                           ======================

    While we initially had far more elaborate plans for mapping LIR to
    ASM, little time dictated that we produce a quick solution.
    Decaf.InstructionSelector implements a naive map from an LIRProgram
    to the corresponding x86_64 assembly string; the corresponding
    specification for the map is structured in a typeclass as

        class ASM a where
            -- | The 'intelasm' function formats the LIR node (pretty-print)
            -- into a gun assembly string.
            gnuasm:: a -> String

            -- | The 'intelasm' function formats the LIR node (pretty-print)
            -- into a intel assembly string.
            intelasm :: a -> String -- turn into a generic tree
    
    Where we only implemented the 'intelasm' function over both the
    SymbolTable and LIRPropgram; this naive map greedily tiles the
    LIRPrograpm with corresponding x86_64 machine instructions; an
    example is as follows:

    genExpr :: LIRReg -> LIRBinExpr -> String
    genExpr reg expr =
      case expr of
          LIRBinExpr op1' LSUB op2' -> mov R10 op1' ++ sep
                                    ++ sub R10 op2' ++ sep
                                    ++ regSave reg

          LIRBinExpr op1' LADD op2' -> mov R10 op1' ++ sep
                                    ++ add R10 op2' ++ sep
                                    ++ regSave reg

    Which takes a more or less linearized stream of LIRInstructions (of
    type LIRInst), and maps it into the corresponding assembly.

    Mosty importantly about this part of the implementation, we regret
    not implementing the map as a monadic traversal of the (as was done
    in Translator); a decision mostly forced on us by the time crunch.
    This decision led to two major complexities, and will need to be
    addressed with a potential rewrite of the code emitter:

        * We were unable to assign and/or generate new state at the
          point of code emission; while it is conceptually ideal to
          separate concerns as such, flaws in our initial assumptions
          about the required structure of the mapping emerged as serious
          implementation limitations; instead, we were constrained to a
          more-or-less direct (1-to-1) mapping from LIR to ASM, even when
          there would be simpler implementations available

        * 
