Dataflow Optimizations (Raeez/Scott)


Although implementing Hoopl in our compiler and writing our first
dataflow pass (CSE) were fairly painful, now that we understand the
ropes, we are hoping to leverage it as much as possible in this final
phase.  It allows us to write basic, independent analyses and then
combine them all at once because of the way it interleaves rewrites
with analysis.

For the final phase, the we intend to implement the following:
    * Rewrite combinators (this is simple)
    * Partial redundancy elimination (leveraging SSA)
    * Sparse conditional constant propagation
    * Dead code elimination 
    * Liveness analysis


After implementing these, our register allocator, and some
instruction-level optimizations, we will be in a better position to
see where our remaining time should be spent.  We may focus on
implementing many dataflow optimizations, or we may spend more time on
parallelizing our output code.



Register allocation (Scott)


We plan to implement a standard graph-coloring register allocator,
based on Chaitin's algorithm and the class lecture.  We will likely
implement a general dependency analysis framework to use with register
allocation and instruction scheduling.  The algorithm will split the
program into webs, as described in lecture, and perform the graph
coloring algorithm with coalescing and simple spill heuristics.  We
plan to pursue register allocation more aggressively than instruction
selection, so we will probably run the register allocator as the final
step in our optimization pipeline.



List Scheduling (Justin)

We will implement the basic instruction scheduling techniques
discussed in class.  Justin works on SAT solvers, so we hope to have
very effective approximation algorithms for list scheduling and other
optimization phases too.



Data Parallelization (Scott/Raeez)

We expect parallelization to be one of the most important optimization
phases, given the many large loops and large arrays used in the test
programs.  We know relatively little about this, however, so we plan
to devote a significant amount of our time to it.  We plan to
implement a loop dependency algorithm similar to the one recently
described in lecture using integer linear programming.  We will
investigate the usefulness of inlining, interprocedural analysis, and
inner loop parallelism in this phase.



Rough Timeline:

First two weeks: Raeez implements core dataflow analyses and peephole
optimizations.  Scott implements register allocator and other dataflow
analysis.  Justin works on instruction scheduling.

Second two weeks: Scott and Raeez implement paralellizing
optimizations.  Justin finishes instruction scheduling and applies SAT
solver techniques to any amenable problems.
