1.

Raeez: Merged the scanners and parsers from week one, implemented
type classes for several common AST features, wrote tree rewrite
functions for the AST and expression trees and Graphviz generation code,
rewrote IR code.

Scott: Implemented monad for symbol table generation/lookup,
implemented semantic checks.

Justin: Set up testing framework, entered all tests, fixed bugs

2. Clarification; we assumed we were allowed free choice in the choice
of internal data structures; beyond this, most of the design choices
fell naturally from a) the fact that we're implementing the compiler in
a pure functional language and b) the fact that we have limited time for
each phase. For example, we decided that we would implement zippers (for
arbitrarily directional tree walks) later on, and instead only
implemented context trees for this section of the compiler.

We also assumed we were able to punt on the implementation of multiple
error reporting out of the parser; as detailed below, in Haskell this is
a hard problem, and we're deferring this implementation until later. At
the moment, our parser reports the first error it finds, and then stops;
this will change in the future.

Finally, we assumed that we could pretty-print/vizualize our trees as we
see fit, and so instead of implementing a complex ascii pretty-printer,
we instead build a graphviz visualizer for our generic trees (and
transformers for our AST to generic trees), and as such we can visualize
symbol tables, the AST, generic trees etc. as images (specifically, our
'grapher' binary outputs PNG files). see the viz/ directory for some
examples.

3.  For the semantic checking phase, we decided to utilizie a High Level
IR (called the HIR for the rest of this document). As a design maxim, we
decided to use Haskell's strengths to our advantage; we created  an algebraic
datatype-constrained Abstract Syntax Tree within the host language (Haskell).
This proved to be a good design choice as we were able to use the incredibly
powerful Hindley Milner type system present in GHC (The Glasgow Haskell Compiler)
to our advantage-we had the guarantee of the compiler and type system to
ensure that our abstract syntax trees were never invalid; as such, we
gained a few hard benefits:
    easy validation of the correctness of our code
    easy validation of the correctness of our tree rewrites
The disadvantages of this design choice became apparent in the need to
implement roughly 30 pattern matches every time we wanted to
modify/inspect/walk the tree in any form.

In addition to a type constrained HIR (where each node is it's own
type), we made extensive use of generic trees (where each node is of
Type Node a); generic trees proved to be incredibly powerful, and *much*
easier to manipulate/rewrite. Again, the difficulty of being new to
Haskell came through, as simple operations (such as numbering the nodes
of a generic tree with unique ID's) took incredibly long to master; once
the learning curve was surmounted, however, we were able to utilize much
of the generic interfaces we had devised to implement fairly powerful
features (such as pretty printing and graphviz tree visualization).

The semantic checks managed by constructing symbol tables over a walk of
the HIR. The design of this walk was monadic, in that a custom Monad keeps
track of the symbol table (represented by it's own generic tree). The
monad encapsulates most common operations on the symbol table: lookup
of identifiers, addition of identifiers, pushing new environments and
automatically removing them, error reporting, and environmental
awareness (for checking returns, breaks, continues). Again, this
powerful design choice resulted in clean code; the disadvantages,
however, revealed themselves in the time it took to design the approach;
very little time was left to testing (as a lot of time was invested into
understanding and designing the custom Monad). All semantic checks were
then implemented in DSL specified by the Monad.

4.  We continued to learn much about haskell.  Just implementing trees
with easy modification/access functions was something of a challenge.
The method settled on was storing a copy of the
tree along with a context (a list of branch choices starting at the
root, going to the current node).  Manipulating the context is easy,
since it's just a list (moving up one level corresponds to taking the
init of the list, for example).  Modifying the data at any single node
requires modifications to all intermediate nodes leading up to the
root of the tree, since the whole structure is immutable, and the
context is just what is needed to do this efficiently. It turns out,
that Zippers are the ideal solution to this problem, where Context Trees
are an acceptable solution for certain operations; the intention is to
implement Zippers in the coming weeks.

Using Context Trees for the symbol table since it is well suited for
performing the primary operations on environments, and it also
automatically gives the full symbol table at the end of the scan.
Implementing the symbol tables in tree form has the nice property of
elegance, and decent lookup times, but we're almost certain that we may
need to investigate more efficient and optimized forms of storage over
the next few weeks.

Continuing on the theme of manipulating trees, it proved far more
elegant to implement a generic tree type (i.e. a tree of any type), and
implement transformers from our custom types (AST/Symbol table etc.) to
this 'generic tree'. Once we had our tree in the 'generic tree' format,
we could utilize our generic algorithms for graphing and analysis; this
proved, in hindsight, to be very useful.

The design decision of implementing graph visualization as an image
instead of ascii pretty printing proved to be a HUGE win, as
implementing tree rewrite rules could be done *visually*; being able to
simply view emerging visual patterns in the various trees we were
working with followed by implementing the tree rewrite rules as pattern
matches using haskell's patterns felt *incredibly* efficient and proved
to be very elegant (tree rewrites never took more then a few lines).

A considerable amount of time was spent rewriting our old type system
to support some of the new features we needed.  We (Raeez and Scott) are
still in the process of mastering the embedding of the structure of our
problems in Haskell's type system, and it seems every problem we solve
yields a more efficient/concise or expressive way to implement the
solutions to (sometimes common) problems. As such, we spent a great deal
of time rewriting our code before launching into the semantic checking
phase; and we anticipate continuing along this practice in the future.
An example of this was in the (late) addition of type classes which
enabled polymorphic treatment of our structured Abstract Syntax Tree;
from this, we gained incredibly elegant tree rewriting code; this,
however, came late in the project-again reflection of our relative
newness to Haskell.

6. Known problems - since it took a long time to get our designs working
(and fantastically, since we embedded a lot of the problem structure in
the type system, the various systems continue to work
first-time-after-first-compile ;-), we were unable to generate an
incredibly comprehensive test suite; Justin was able to ensure that we
matched the basic feature set, but a lot of the work we have up ahead is
individual constraint and unit tests for each functional component of
our compiler's pipeline.

Additionally, we uncovered a bug in our scanner; as a result of
utilizing Haskell's Parsec along with it's position tracking State
Monady, we discovered that Parsec treats tab characters as having a
length of 8 (eight) charactersâ€”thus, whenever our scanner discovers
a tab character, it (incorrectly) jumps forward by 8 characters,
as opposed to the length of one token (in this case, the corresponding
length should be one character). We are in full knowledge of this bug,
and know that fixing it requires rolling our own monadic state tracking
design into our error recovery mechanism (instead of utilizing Parsec's,
which has semantics different from that which we initially expected). We
intend to follow by implementing this design, as it will be immediately
generalizable, and hence applicable to our parser. It is for this reason
that our parser does not utilize error recovery (i.e. it only reports
the first error it encounters) at this point, as once we have the more
generic state tracking and error recovery mechanism (a design which we
all found challenging in the last phase), we can utilize that in the
parser, which-at this point-is also designed with Parsec.
